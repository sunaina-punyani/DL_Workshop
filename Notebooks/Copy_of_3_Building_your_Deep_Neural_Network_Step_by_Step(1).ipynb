{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 3. Building your Deep Neural Network - Step by Step.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "H5k2neJd0NEV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building your Neural Network: Step by Step\n",
        "\n",
        "Welcome to the session(part 2 of 3)! \n",
        "\n",
        "- In this notebook, you will implement all the functions required to build a neural network.\n",
        "- In the next assignment, you will use these functions to build a deep neural network with as many layers as you want!\n",
        "\n",
        "**After this assignment you will be able to:**\n",
        "- Use non-linear units like sigmoid, tanh , ReLU to improve your model\n",
        "- Build a deeper neural network (with more than 1 hidden layer)\n",
        "- Implement an easy-to-use neural network \n",
        "\n",
        "**Notation**:\n",
        "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
        "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
        "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
        "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
        "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
        "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "metadata": {
        "id": "lqxhLT9V0NEc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 - Packages\n",
        "\n",
        "Let's first import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the main package for scientific computing with Python.\n",
        "\n",
        "\n",
        "\n",
        "*  -[pandas](https://pandas.pydata.org/) is the package used for operations on data.\n",
        "\n",
        "\n",
        "- np.random.seed(2) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed. "
      ]
    },
    {
      "metadata": {
        "id": "m62Sf3tX0NEv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Outline of the Assignment\n",
        "\n",
        "To build your neural network, you will be implementing several \"helper functions\". These helper functions will be used  to build a two-layer neural network. Each small helper function you will implement will have detailed instructions that will walk you through the necessary steps. Here is an outline of this assignment, you will:\n",
        "\n",
        "- Initialize the parameters for a two-layer network and for an $L$-layer neural network.\n",
        "- Implement the forward propagation module.\n",
        "     - Complete the LINEAR part of a layer's forward propagation step.\n",
        "     - Write and use activation functions.\n",
        "     - Combine the previous two steps into a new [LINEAR->ACTIVATION] forward function.\n",
        "     \n",
        "- Compute the loss.\n",
        "- Implement the backward propagation module .\n",
        "    - Complete the LINEAR part of a layer's backward propagation step.\n",
        "    - Combine the previous two steps into a new [LINEAR->ACTIVATION] backward function.\n",
        "   \n",
        "\n",
        "<img src=\"Notebook_images/final outline.png\" style=\"width:800px;height:500px;\">\n",
        "<caption><center> **Figure 1**</center></caption><br>\n",
        "\n",
        "\n",
        "**Note** that for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. The cached values are useful for computing gradients. In the backpropagation module you will then use the cache to calculate the gradients. This assignment will show you exactly how to carry out each of these steps. "
      ]
    },
    {
      "metadata": {
        "id": "KfWXv0SR0NE0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 - Initialization\n",
        "\n",
        "You will write a helper functions that will initialize the parameters for your model. The initialize_parameters function will be used to initialize parameters for a two layer model. \n",
        "\n",
        "### 3.1 - 2-layer Neural Network\n",
        "\n",
        "**Exercise**: Create and initialize the parameters of the 2-layer neural network.\n",
        "\n",
        "**Instructions**:\n",
        "- The model's structure is: *LINEAR ->TANH-> LINEAR -> SIGMOID*. \n",
        "- Use random initialization for the weight matrices. Use `np.random.randn(shape)*0.01` with the correct shape.\n",
        "- Use zero initialization for the biases. Use `np.zeros(shape)`."
      ]
    },
    {
      "metadata": {
        "id": "Q3bmK0it0NF6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 - Forward propagation module\n",
        "\n",
        "### 4.1 - Linear Forward and activation\n",
        "Now that you have initialized your parameters, you will do the forward propagation module. You will start by implementing some basic functions that you will use later when implementing the model. You will complete three functions in this order:\n",
        "\n",
        "- LINEAR\n",
        "- LINEAR -> ACTIVATION where ACTIVATION will be either tanh or Sigmoid. \n",
        "\n",
        "\n",
        "The linear forward module (vectorized over all the examples) computes the following equations:\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
        "\n",
        "where $A^{[0]} = X$. \n",
        "\n",
        "**Exercise**: Build the linear part of forward propagation.\n",
        "\n",
        "**Reminder**:\n",
        "The mathematical representation of this unit is $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$. You may also find `np.dot()` useful. If your dimensions don't match, printing `W.shape` may help."
      ]
    },
    {
      "metadata": {
        "id": "kgrZr1_o0NGb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For more convenience, you are going to group two functions (Linear and Activation) into one function (LINEAR->ACTIVATION). Hence, you will implement a function that does the LINEAR forward step followed by an ACTIVATION forward step.\n",
        "\n",
        "**Exercise**: Implement the forward propagation of the *LINEAR->ACTIVATION* layer. Mathematical relation is: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ where the activation \"g\" can be sigmoid() or tanh(). "
      ]
    },
    {
      "metadata": {
        "id": "P83Xe5UZ0NG4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note**: In deep learning, the \"[LINEAR->ACTIVATION]\" computation is counted as a single layer in the neural network, not two layers. "
      ]
    },
    {
      "metadata": {
        "id": "ySdVy7Dr0NHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Great! Now you have a full forward propagation that takes the input X and outputs a row vector $A^{[L]}$ containing your predictions. It also records all intermediate values in \"caches\". Using $A^{[L]}$, you can compute the cost of your predictions."
      ]
    },
    {
      "metadata": {
        "id": "oHOBrFWO0NHe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5 - Cost function\n",
        "\n",
        "Now you will implement forward and backward propagation. You need to compute the cost, because you want to check if your model is actually learning.\n",
        "\n",
        "**Exercise**: Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$\n"
      ]
    },
    {
      "metadata": {
        "id": "E0X84V030NH9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6 - Backward propagation module\n",
        "\n",
        "Just like with forward propagation, you will implement helper functions for backpropagation. Remember that back propagation is used to calculate the gradient of the loss function with respect to the parameters. \n",
        "\n",
        "**Reminder**: \n",
        "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
        "<caption><center> **Figure 3** : Forward and Backward propagation for *LINEAR->TANH->LINEAR->SIGMOID* <br> *The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.*  </center></caption>\n",
        "\n",
        "<!-- \n",
        "For those of you who are expert in calculus (you don't need to be to do this assignment), the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n",
        "\n",
        "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
        "\n",
        "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n",
        "\n",
        "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
        "\n",
        "This is why we talk about **backpropagation**.\n",
        "!-->\n",
        "\n",
        "Now, similar to forward propagation, you are going to build the backward propagation in three steps:\n",
        "- LINEAR backward\n",
        "- LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the tanh or sigmoid activation\n"
      ]
    },
    {
      "metadata": {
        "id": "HUcm-C-b0NIE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.1 -backward_propogation(Linear-activation backward)\n",
        "\n",
        "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
        "\n",
        "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
        "\n",
        "<img src=\"Notebook_images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
        "<caption><center> **Figure 4** </center></caption>\n",
        "\n",
        "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
      ]
    },
    {
      "metadata": {
        "id": "YbDrSE0S0NII",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Use the 3 formulas above to implement linear_backward()."
      ]
    },
    {
      "metadata": {
        "id": "Rgt0gKAu0NIs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "If $g(.)$ is the activation function, \n",
        "whule backward_propagation you compute compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
        "\n",
        "**Exercise**: Implement the backpropagation for the *LINEAR->ACTIVATION* layer."
      ]
    },
    {
      "metadata": {
        "id": "G9yBaWA00NJW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.4 - Update Parameters\n",
        "\n",
        "In this section you will update the parameters of the model, using gradient descent: \n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
        "\n",
        "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "
      ]
    },
    {
      "metadata": {
        "id": "BSqNmX860NJX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Implement `update_parameters()` to update your parameters using gradient descent.\n",
        "\n",
        "**Instructions**:\n",
        "Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$. \n"
      ]
    },
    {
      "metadata": {
        "id": "t2s6CcSy0NJp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 7 - Conclusion\n",
        "\n",
        "Congrats on implementing all the functions required for building a neural network! \n",
        "\n"
      ]
    }
  ]
}